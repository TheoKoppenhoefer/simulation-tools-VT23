
\documentclass{report}
% alternatives: scrartcl, article or report


%%%%% PACKAGES

% small tweaks and nicer typography
\usepackage{microtype}
\usepackage{hyperref}

% dealing with figures
\usepackage{subcaption}
\usepackage{wrapfig}

% basic math stuff
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{tikz-cd}
\usepackage{cancel}
\usepackage{cases}
\usepackage{dsfont}

% tikz
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{positioning}
%\usetikzlibrary{patterns}
%\usetikzlibrary{babel}
\tikzset{>=stealth}
\usepackage{wrapfig}

% code
\usepackage{listings}
\usepackage{pythonhighlight}

%%%% Graphics %%%%%

%\graphicspath{{Plots/}}

%\newcommand{\tikzmark}[3][]{\tikz[remember picture,baseline] \node [anchor=base,#1](#2) {$#3$};}

%\usepackage{booktabs}
%\usepackage{bm}
%\usepackage{minted}

% for inkscape images
%\usepackage{pdftricks}
%\begin{psinputs}
%   \usepackage{pstricks}
%   \usepackage{multido}
%\end{psinputs}
%\usepackage[pdf]{pstricks}
%\usepackage{import}


\usepackage[backend=bibtex]{biblatex}
\addbibresource{mybib.bib}


%%%%% CONFIGURATION

% prevents automatic line breaks inside of equations
% since it looks bad
\binoppenalty = \maxdimen
\relpenalty   = \maxdimen

% theorem-like environments
\newcounter{everything}
\newtheorem{corollary}[everything]{Corollary}
\newtheorem{lemma}[everything]{Lemma}
\newtheorem{proposition}[everything]{Proposition}
\newtheorem{theorem}[everything]{Theorem}
\newtheorem*{claim}{Claim}
\newtheorem*{given}{Given}


%%%%% CUSTOM COMMANDS

% real numbers via \R
% complex numbers via \C
% general field via \K
\def\C{\mathbb{C}}
\def\R{\mathbb{R}}
\def\K{\mathbb{K}}
\def\Q{\mathbb{Q}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\def\H{\mathbb{H}}
\def\e{\varepsilon}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand{\bA}{\mathbb{A}}
\newcommand{\bB}{\mathbb{B}}
\newcommand{\bC}{\mathbb{C}}
\newcommand{\bD}{\mathbb{D}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\bH}{\mathbb{H}}
\newcommand{\bI}{\mathbb{I}}
\newcommand{\bJ}{\mathbb{J}}
\newcommand{\bK}{\mathbb{K}}
\newcommand{\bL}{\mathbb{L}}
\newcommand{\bM}{\mathbb{M}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bO}{\mathbb{O}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bS}{\mathbb{S}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bU}{\mathbb{U}}
\newcommand{\bV}{\mathbb{V}}
\newcommand{\bW}{\mathbb{W}}
\newcommand{\bX}{\mathbb{X}}
\newcommand{\bY}{\mathbb{Y}}
\newcommand{\bZ}{\mathbb{Z}}

\newcommand{\dif}[1]{\,\mathrm{d} #1}
%\newcommand{\norm}[1]{\lVert #1 \rVert}
%\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\bnorm}[1]{\left\lVert #1\right\rVert}
\newcommand{\vii}[2]{\ensuremath{\begin{bmatrix}#1 \\ #2 \end{bmatrix}}}
\newcommand{\mii}[4]{\ensuremath{\begin{bmatrix}#1&#2 \\ #3&#4 \end{bmatrix}}}
\newcommand{\mc}[1]{\mathcal{#1}}

\newcommand{\one}{\mathds{1}}
\newcommand{\bigO}{\mathcal{O}}


%%%%%%%%%%    Math operators    %%%%%%%%%%%%%%%%%%%%%%%%%%%

\DeclareMathOperator{\Id}{Id}             % identity morphism
% \DeclareMathOperator{\ker}{ker}           % kernel
\DeclareMathOperator{\rg}{rg}             % image
\DeclareMathOperator{\defekt}{def}             % defect
\DeclareMathOperator{\im}{im}             % image
\DeclareMathOperator{\Hom}{Hom}           % homomorphisms
\DeclareMathOperator{\End}{End}           % endomorphisms
\DeclareMathOperator{\Span}{Span}         % linear span
\DeclareMathOperator{\grad}{\nabla}         % gradient
\DeclareMathOperator{\diam}{diam}         % gradient
\DeclareMathOperator{\Tr}{Tr}       	  % trace
\DeclareMathOperator{\diver}{Div}			% divergence
\DeclareMathOperator{\supp}{supp}			% support
\DeclareMathOperator{\dist}{dist}			% distance
\DeclareMathOperator{\inter}{int}			% interiour
\DeclareMathOperator{\epi}{epi}			% epigraph
\DeclareMathOperator{\hyp}{hyp}			% hypograph
\DeclareMathOperator{\Lip}{Lip}			% lipschitz konstant
\DeclareMathOperator{\graph}{graph}			% graph
\DeclareMathOperator{\sgn}{sgn}			% sign
\DeclareMathOperator{\BMO}{BMO}			% BMO
\DeclareMathOperator{\mean}{mean}			% BMO
%\DeclareMathOperator{\B}{B}			% BMO

% inner product (scalar product) via \inner{v, w}
% norm via \norm{x}
% absolute value via \abs{x}
% use the star-version for automatic scaling
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

% \vect{ x // y // z } for a column vector with entries x, y, z
% similarly for larger vectors
% in this code:  1 = number of arguments
%               #1 = first argument
\newcommand{\vect}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\commentops}[2]{\stackrel{\mathclap{#1}}{#2}}


% \conj{z} for complex conjugation
\newcommand{\conj}{\overline}

%counter of current constant number:    
\newcounter{constant} 
%defines a new constant, but does not typeset anything:
\newcommand{\newconstant}[1]{\refstepcounter{constant}\label{#1}} 
%typesets named constant:
\newcommand{\useconstant}[1]{c_{\ref{#1}}}

%%%%%%% GENERAL STYLE %%%%%%%%%%%%%%%%%%

%\setcounter{tocdepth}{3}


%%%%% TITLE PAGE

%\subject{Simulation Tools, VT23}
\title{Project Reports}
\author{Salvador Castagnino, Theo Koppenhöfer}
\date{\today}


%%%%% The content starts here %%%%%%%%%%%%%


\begin{document}

\maketitle

\chapter*{Project 1}

\section*{Introduction}

This report and the code belonging to it can be found online under \cite{Repository}.

\subsection*{The Benchmark}

In the following we use the model of a pendulum attached to a rod which is elastic in the radial direction as described in Task 1. The situation is depicted in figure \ref{dr:Pendulum}.

\begin{wrapfigure}{r}{0.43\textwidth}
\centering
\input{../Drawings/pendulum.pdf_tex}
\caption{The pendulum}
\label{dr:Pendulum}
\end{wrapfigure}

This problem leads to the formulation as an ODE
\begin{align*}
	\vect{y_1 \\ y_2 \\ y_3 \\ y_4}' = \vect{y_3 \\ y_4 \\ -y_1\lambda(y_1,y_2) \\ -y_2\lambda(y_1,y_2)-1}
\end{align*}
with
\begin{align*}
	\lambda(y_1,y_2)=k\frac{\norm{(y_1,y_2)}-1}{\norm{(y_1,y_2)}}\,.
\end{align*}
The plot of a numerical solution to this problem for $k=1$ can be seen in figures \ref{pl:StateTime1} \ref{pl:PhasePortrait1} and \ref{pl:PolarPlot1}.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_1}
\caption{State in dependence of time.}
\label{pl:StateTime1}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_2}
\caption{Path traced out by pendulum.}
\label{pl:PhasePortrait1}
\end{minipage}
\end{figure}

We can calculate the potential, kinetic and approximate the elastic energy with the formulas
\begin{align*}
	E_{\text{pot}}=1+y_2
	\qquad E_{\text{kin}}=\frac{\norm{(y_3,y_4)}^2}{2}
	\qquad E_{\text{elast}}=k\frac{(\norm{(y_1,y_2)}-1)^2}{2}\,.
\end{align*}
Adding these up we get the approximate total energy
\begin{align*}
	E_{\text{tot}}=E_{\text{pot}}+E_{\text{kin}}+E_{\text{elast}}\,.
\end{align*}
We expect the approximate total energy to be constant which indeed can be seen in Figure \ref{pl:EnergyPlot1} for that previously calculated numerical solution.
Because of this property we can use the variance of the approximate total energy as an index to measure the stability of the method. 
In the ideal world this index almost vanishes.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_0}
\caption{Polar coordinates.}
\label{pl:PolarPlot1}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_3}
\caption{Energy plot}
\label{pl:EnergyPlot1}
\end{minipage}
\end{figure}

\section*{Testing Explicit Methods}

For linear problems, explicit methods present a much reduced stability region which dictates the possible step sizes for that specific method.
For the problem of the elastic pendulum, approximated by explicit methods, when the value of $k$ is increased we are expected to see the approximation blow up showing oscillations of unbounded amplitude.
This unstable behavior will be attenuated by reducing the value of the step $h$.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Drawings/Runge_Kutter_stability_regions}
\caption{Stability regions of the Runge-Kutta-methods, taken from \cite{Stab_RK}[p.238]}
\end{minipage}
\end{figure}

The problem was simulated using Explicit Euler and RK4.
All the experiments in this section are simulated on the domain $[0,20]$ and have initial the initial value $y_0 = [1.1, 0, 0, 0]$ if not otherwise stated.
The graphs are presented in polar coordinates where $r$ refers to the length of the spring and $\theta$ refers to the angle conformed between the pendulum and the vertical axis.

It can be observed that for a step size of $h=0.01$ Explicit Eurler (Figure \ref{exp_euler_k=50_h=0.01_c}) already shows instability for values of $k=50$ while RK4 (Figure \ref{rk4_h=0.01_k=3000_c}) with that same step size remains stable for values up to $k=3000$.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/ExpEuler/exp_euler_k=50_h=0.01_c}
\caption{Explicit Euler $h=0.01$ $k=50$}
\label{exp_euler_k=50_h=0.01_c}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/RK4/rk4_h=0.01_k=3000_c}
\caption{RK4 $h=0.01$ $k=3000$}
\label{rk4_h=0.01_k=3000_c}
\end{minipage}
\end{figure}
	
For Explicit Euler (Figure \ref{exp_euler_k=50_h=0.001_c_2}), by keeping the value of $k$ constant and reducing the step size by a decimal place we can see how the instability is attenuated presenting a similar amplitude over time.
It takes a much larger step size and spring constant for RK4 to become unstable (Figure \ref{rk4_h=0.1_k=975_c}), once unstabilized it’s growth is much more rapid than Explicit Euler’s and it does so without oscillating.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/ExpEuler/exp_euler_k=50_h=0.001_c}
\caption{Explicit Euler $h=0.001$ $k=50$}
\label{exp_euler_k=50_h=0.001_c_2}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/RK4/rk4_h=0.1_k=975_c}
\caption{RK4 $h=0.1$ $k=975$}
\label{rk4_h=0.1_k=975_c}
\end{minipage}
\end{figure}

It is interesting to observe that the oscillation of the spring is rapidly dumped when using RK4 (Figure \ref{rk4_h=0.1_k=300_c}), a behavior similar to that presented by implicit methods.
This behavior cannot be observed in the other explicit methods.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/RK4/rk4_h=0.1_k=300_c}
\caption{RK4 $h=0.1$ $k=300$}
\label{rk4_h=0.1_k=300_c}
\end{minipage}
\end{figure}

\section*{Testing Implicit Methods}

Opposite to the case of explicit methods, for linear problems implicit methods count with an extensive stability region which does not make their stability dependent on the value of the step $k$.
The problem was simulated using Implicit Euler, BDF2 with Fixed Point as corrector and BDFk with Newton as corrector for $k$ between $1$ and $4$. All the following experiments take as initial value $y_0 = [1.1, 0, 0, 0]$ and have $[0, 20]$ for time domain.
It is interesting to see how the oscillation of the spring decays for implicit methods. This decay can be attenuated by reducing the step size or accelerated by increasing the value of the spring constant.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/ImpEuler/imp_euler_h=0.01_k=50_c}
\caption{Implicit Euler $h=0.01$ $k=50$}
\label{imp_euler_h=0.01_k=50_c}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/BDF2/bdf2_h=0.01_k=500_c}
\caption{BDF2-Fixed Point $h=0.001$ $k=500$}
\label{exp_euler_k=50_h=0.001_c}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/BDF2/bdf2_h=0.01_k=1000_c}
\caption{BDF2-Fixed Point $h=0.01$ $k=1000$}
\label{bdf2_h=00.1_k=1000_c}
\end{minipage}
\end{figure}

The method BDFk with Newton presents a decay in the spring oscillation as the other methods do.
However, it also shows decay of the pendulum oscillation which cannot be observed in the other implicit methods.
To better observe this decay (Figure \ref{bdf2_h=0.01_h=100_tf=100} and Figure \ref{bdf4_h=0.01_h=100_tf=100.png}) the domain is increased to $[0, 100]$.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/BDFk/bdf2_h=0.01_h=100_tf=100}
\caption{BDF2-Newton $h=0.01$ $k=100$}
\label{bdf2_h=0.01_h=100_tf=100}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/BDFk/bdf4_h=0.01_h=100_tf=100}
\caption{BDF4-Newton $h=0.01$ $k=100$}
\label{bdf4_h=0.01_h=100_tf=100.png}
\end{minipage}
\end{figure}

It is interesting to see how the relation between the speed of decay of the spring oscillation in inversely proportional to the size of the stability region of the methods tested.


\newpage
\newpage


\section*{Testing CVODE}
\subsection*{A first test series}

In the first specific test of CVODE we solve our toy problem for increasing $k$. Here we switch between the BDF and Adam-Moultons discretisation method. We also vary the \pyth{maxorder} parameter for both methods.
A higher $k$ reflects a problem which is more stiff. As a stiff problem requires smaller steps the number of steps \pyth{nsteps} increases as $k$ increases which  can be seen in figure \ref{pl:nsteps1}. As the number of function evaluations per stepsize \pyth{nfcns/nsteps} hovers slightly above 1 for all methods (c.f.\ figure \ref{pl:nfcns_nsteps1}) the number of function evaluations increase proportionally to \pyth{nsteps} with $k$ as can be seen in figure \ref{pl:nfcns1}. There is however a difference in how many steps each method needs. The BDF-method requires in general more steps than the Adams-Moulton method. And the general trend is that the number of steps increases as \pyth{maxord} is reduced. 

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_200}
\caption{\pyth{nsteps} in relation to the parameter $k$.}
\label{pl:nsteps1}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_201}
\caption{\pyth{nfuncs} in relation to the parameter $k$.}
\label{pl:nfcns1}
\end{minipage}
\end{figure}

From figure \ref{pl:njacs_nsteps1} it can be seen that the number of jacobian evaluations stays roughly constant and happens roughly every 5th step. The number \pyth{nerrfails/steps} stays roughly constant in dependence of $k$ though the general tendency is that it is smaller the lower \pyth{maxord} is set. This makes sense because a lower \pyth{maxord} means there are fewer possibilities for the method order and hence fewer changes of order. In figure \ref{pl:stability1} we see a difference in how much the methods obey the principles of energy conservation. One can see that for growing $k$ the result tends to be further away from physical reality. Once again the methods with higher \pyth{maxord} do better with the exception of the BDF method where for some reason a \pyth{maxord} of 4 performs best.


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_210}
\caption{\pyth{nfcns/nsteps} in relation to the parameter $k$.}
\label{pl:nfcns_nsteps1}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_211}
\caption{\pyth{njacs/nsteps} in relation to the parameter $k$.}
\label{pl:njacs_nsteps1}
\end{minipage}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_212}
\caption{\pyth{nerrfails/nsteps} in relation to the parameter $k$.}
\label{pl:nerrfails_nsteps1}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_204}
\caption{Variance of the energy in relation to the parameter $k$.}
\label{pl:stability1}
\end{minipage}
\end{figure}

This test confirms once again that a stiffer Problem needs more function evaluations in CVODE.
Perhaps surprisingly the Adams-Moulton-method seems to perform better on this problem.
This experiment also highlights that a lower \pyth{maxord} parameter tends to be more computationally expensive though it reduces the number of error test failures \pyth{nerrfails}.

\subsection*{Testing the parameter rtol}
We now test the influence of the parameter \pyth{rtol} on the methods BDF and Adams-Moulton. For this we set $k=10^3$ and keep all other parameters on their default values. The results can be seen in figures \ref{pl:nsteps2} to \ref{pl:stability2}. We note that as \pyth{rtol} increases the number of steps decreases (c.f.\ figure \ref{pl:nsteps2}). If one compares the variance of the energy for $k\approx10^3$ in figure \ref{pl:stability1} with the variance of the energy in figure \ref{pl:stability2} one sees that changing the \pyth{rtol} parameter from the default makes the result significantly worse in terms of energy conservation.

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_300}
\caption{\pyth{nsteps} in relation to the parameter \pyth{rtol}.}
\label{pl:nsteps2}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_310}
\caption{\pyth{nfcns/nsteps} in relation to the parameter \pyth{rtol}.}
\label{pl:nfcns_nsteps2}
\end{minipage}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_311}
\caption{\pyth{njacs/nsteps} in relation to the parameter \pyth{rtol}.}
\label{pl:njacs_nsteps2}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_312}
\caption{\pyth{nerrfails/nsteps} in relation to the parameter \pyth{rtol}.}
\label{pl:nerrfails_nsteps2}
\end{minipage}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_304}
\caption{Variance of the energy in relation to the parameter \pyth{rtol}.}
\label{pl:stability2}
\end{minipage}
\end{figure}

\subsection*{Testing the parameter atol}

If we test the \pyth{atol} parameter on the Adams and Newton method analogously to the test of the \pyth{rtol} parameter we once again get a variance of the energy that is significantly above the value for the method in which we did not specify this value as can be seen in Figure \ref{pl:stability3}. In either case we observe that fixing the tolerance seems to come at the cost of energy conversation as is dramatically visualised in Figures \ref{pl:energy2} and \ref{pl:energy3}.


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_404}
\caption{Variance of the energy in relation to the parameter \pyth{atol}.}
\label{pl:stability3}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_450}
\caption{Energy plot for $k=10^3$ with \pyth{atol=1E-2}.}
\label{pl:energy2}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Task4/Figure_451}
\caption{Energy plot for $k=10^3$.}
\label{pl:energy3}
\end{minipage}
\end{figure}

All in all we see that none of the (admittedly crude) tweaking of the parameters improved the performance of CVODE. To the contrary, most changes worsened the performance. The choice of the discretisation method on the other hand did make a big difference and the performance for solving the toy problem could be improved by switching from the default BDF method.


\chapter*{Project 2}

In this project we used the implementation of the seven body mechanism as described in \cite{HW_SolvingODEs} to test Assimulo's implicit solvers. The problem formulation leads to an index 3 problem of the form
\begin{align}
	M(q)\,q''&= f(q,q')-G(q)^\top\,\lambda\label{eq:sevenBody} \\
	0 &= g(q) \label{eq:condition_indx3}
\end{align}
where $q\in\R^7$, $\lambda\in\R^6$ and $G=Dg$. If we differentiate condition \eqref{eq:condition_indx3} we obtain the index 2 condtition
\begin{align*}
	0&=G(q)\,q'
\end{align*}
and differentiating this again we obtain the index 1 formulation
\begin{align*}
	0&= \partial_q^2g(q)\,(q',q')+G(q)\,q''\,.  \label{eq:condition_indx1}
\end{align*}
Note that condition \eqref{eq:condition_indx1} and \eqref{eq:sevenBody} can be uniquely solved for $q''$ and $\lambda$ and we then obtain an ODE in the explicit formulation.
For the implementation we rewrite this second order system as a first order system by the usual trick of introducing the variable $v=q'$ so that in the implicit formulation the problem depends on the variable
\begin{align*}
	y = \vect{q \\ v \\ \lambda}
\end{align*}

A plot of the solution of the index 1 formulation can be seen in Figures \ref{pl:indx3_soln_angles} to \ref{pl:indx3_soln_lambdas}.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_510}
\caption{The angles of the solution to the index 2 problem.}
\label{pl:indx3_soln_angles}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_511}
\caption{The angle speed of the solution to the index 2 problem.}
\label{pl:indx3_soln_anglesdot}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_512}
\caption{The Lagrange parameter of the solution to the index 2 problem.}
\label{pl:indx3_soln_lambdas}
\end{minipage}
\end{figure}

\section*{Generation of consistent initial values}

Given the restrictions imposed over the system the generation of initial values is not a trivial task. To do this we follow the steps presented in the literature, we start with $q$ and $v$.

First we take $\theta=0$ which can be done given that the system is undetermined and we assume a solution exists (this makes sense as a physical model of the system was presented in class). We then use \textit{Newton Iteration} to solve the equations obtaining values for the remaining angles.
We also take the initial value of $v$ to be $0$ as we assume the system starts at rest. Now for $w$ and $\lambda$ using the Index 1 formulation we have to solve a linear system which is presented in the literature. Doing this we get the values given in tables \ref{tb:initial_accelerations}-\ref{tb:initial_lambdas}.


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.3\textwidth}
\centering
\input{../Plots/Tables/Initial_Angles.tex}
\caption{Consistent initial angles}
\label{tb:initial_angles}
\end{minipage}
\hfill
\begin{minipage}[b]{0.3\textwidth}
\centering
\input{../Plots/Tables/Initial_Accelerations.tex}
\caption{Consistent initial accelerations}
\label{tb:initial_accelerations}
\end{minipage}
\hfill
\begin{minipage}[b]{0.3\textwidth}
\centering
\input{../Plots/Tables/Initial_Lambdas.tex}
\caption{Consistent initial lambdas}
\label{tb:initial_lambdas}
\end{minipage}
\end{figure}

Many of the values are minute but non-zero. This is due to rounding errors, but in theory these small values equal zero.

\section*{A comparison of the index 1, 2 and 3 formulations}

We now would like to compare the solutions of the various formulations. To calculate the solutions we used the IDA solver (TODO: hyperlink). However, to get the problem to run we set the \pyth{atol} parameter to the large number \pyth{1E5} and the \pyth{algvar} parameter to \pyth{False} for the algebraic variable $\lambda$ and for $v$. These settings remain unchainged and in the following we only vary the index of the problem. We can see in the figures \ref{pl:indx1_solndiff_angles} to \ref{pl:indx1_solndiff_lambdas} the difference of the index 1 solution subtraced from the index 3 solution. In the figures \ref{pl:indx2_solndiff_angles} to \ref{pl:indx2_solndiff_lambdas} we see the index 2 solution subtracted from the index 3 solution. As expected we see that the difference the solutions grows as time progresses.


\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_530}
\caption{The difference of angles of the index 1 and the index 3 solution.}
\label{pl:indx1_solndiff_angles}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_531}
\caption{The difference in angle speeds of the index 1 and the index 3 solution.}
\label{pl:indx1_solndiff_anglesdot}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_532}
\caption{The difference of lambdas of the index 1 and the index 3 solution.}
\label{pl:indx1_solndiff_lambdas}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_540}
\caption{The difference of angles of the index 2 and the index 3 solution.}
\label{pl:indx2_solndiff_angles}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_541}
\caption{The difference in angle speeds of the index 2 and the index 3 solution.}
\label{pl:indx2_solndiff_anglesdot}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_542}
\caption{The difference of lambdas of the index 2 and the index 3 solution.}
\label{pl:indx2_solndiff_lambdas}
\end{minipage}
\end{figure}

Rather unexpectedly however the difference of the index 1 to the index 3 solution is in general greater than the difference of the index 2 to the index 3 solutions. Also unexpectedly these differences are noticable in the plots of the Lagrange parameter $\lambda$ as shown in figures \ref{pl:indx1_soln_lambdas}, \ref{pl:indx2_soln_lambdas} and \ref{pl:indx3_soln_lambdas}. Here we see that the solution becomes increasingly rough as the index increases.


\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_515}
\caption{The Lagrange parameter of the index 2 problem.}
\label{pl:indx1_soln_lambdas}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_518}
\caption{The Lagrange parameter of the index 3 problem.}
\label{pl:indx2_soln_lambdas}
\end{minipage}
\end{figure}


The performance of the IDA solver for the various indexes can be seen in figures \ref{pl:nsteps_indx123} to \ref{pl:nerrfails_nsteps_indx123}. We see figure \ref{pl:nsteps_indx123} that the number of steps of the solver increases with the index. As the number of function evaluations per step stays roughly constant by figure \ref{pl:nfcns_nsteps_indx123} this means that the number of function evaluations increases with the index. One other other notable statistic regards the number of error test failures for the different problems which can be seen in figure \ref{pl:nerrfails_nsteps_indx123} where we see a larger difference between the problems though this is probably not statistically significant as the total number of error test failures is approximately a dozen.


\begin{figure}[h]
\centering
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_600}
\caption{\pyth{nsteps} in relation to the index.}
\label{pl:nsteps_indx123}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_610}
\caption{\pyth{nfcns/nsteps} in relation to the index.}
\label{pl:nfcns_nsteps_indx123}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_603}
\caption{\pyth{nerrfails} in relation to the index.}
\label{pl:nerrfails_indx123}
\end{minipage}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_612}
\caption{\pyth{nerrfails/nsteps} in relation to the index.}
\label{pl:nerrfails_nsteps_indx123}
\end{minipage}
\end{figure}

\section*{Dependence on the parameters \pyth{algvar} and \pyth{atol}}

As previously indicated the IDA solver will throw an error
\begin{python}
assimulo.solvers.sundials.IDAError: 'Convergence test failures occurred too many times during one internal time step or minimum step size was reached. At time 0.000000.'
\end{python}
This can be resolved by declaring the entries of $y$ corresponding to $v$ and or $\lambda$ to be algebraic variables with the parameter \pyth{algvar} and to set the parameter \pyth{atol} to a large variable. In the following we would like to check how these parameters impact the performance of the solver in the case of the index 1 formulation. For this we denote by \pyth{algvar_v} and \pyth{algvar_lambda} the value of the \pyth{algvar} parameter for $v$ and $\lambda$. The default value of \pyth{algvar} is set to \pyth{True}. Analogously we denote the components of \pyth{atol} corresponding to $v$ and $\lambda$ with \pyth{atol_v} and \pyth{atol_lambda} and set the default value to \pyth{1E-6}. We now run a series of 5 experiments as depicted in table \ref{tb:indx1_experiment_params}.


\begin{figure}[h]
\centering
\input{../Plots/Tables/Overview_Index1Experiment.tex}
\caption{Parameters in the experiments}
\label{tb:indx1_experiment_params}
\end{figure}

As all experiments deliver more or less the same result we will be comparing the statistics of IDA as depicted in figures \ref{pl:nsteps_indx1} to \ref{pl:nerrfails_nsteps_indx1}. Once again we observe in figure \ref{pl:nfcns_nsteps_indx1} that the total number of function evaluations is roughly proportional to the number of steps needed. Here experiments 1 and 4 stick out for requiring comparatively more function evaluations per step. However in figure \ref{pl:nsteps_indx1} we see that these are also precisely the experiments in which the total number of steps taken is by far the least. Experiments 1 and 4 are also precisely those experiments that have the most stringent requirements on the $v$ part of $y$. Both have set \pyth{atol_v=1E-6} and declare $v$ to not be an algebraic variable. We see in figures \ref{pl:njacs_indx1} and \ref{pl:nerrfails_nsteps_indx1} that experiment 0 is an outlier in requiring comparitively many jacobian evaluations and having relatively few error test failures.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_700}
\caption{\pyth{nsteps} of the experiments.}
\label{pl:nsteps_indx1}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_701}
\caption{\pyth{nfcns}  of the experiments.}
\label{pl:nfcns_indx1}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_702}
\caption{\pyth{njacs}  of the experiments.}
\label{pl:njacs_indx1}
\end{minipage}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_710}
\caption{\pyth{nfcns/nsteps} of the experiments.}
\label{pl:nfcns_nsteps_indx1}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_711}
\caption{\pyth{njacs/nsteps} of the experiments.}
\label{pl:njacs_nsteps_indx1}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project2_main/Figure_712}
\caption{\pyth{nerrfails/nsteps} of the experiments.}
\label{pl:nerrfails_nsteps_indx1}
\end{minipage}
\end{figure}

\section*{Using an explicit method}

As part of the final task we used an explicit RK4 method to solve the index 1 problem. As a result of the method exploding for $h=0.01$, the default step value, the method was tested for various values of $h$.

In particular in Figure \ref{pl:L2_norm_explicit} the $L_2$ norm of each angle over time is plotted with respect to $h \in [0.001, 0.002)$ with $\Delta h = 5\mathrm{e}{-6}$.

\begin{figure}[h]
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/RK4_Proj2/L2_norm_different_k.pdf}
\caption{Value of $L_2$ norms depending on $h$}
\label{pl:L2_norm_explicit}
\end{minipage}
\end{figure}


The explicit method can then be tested with individual step sizes and as expected, the method explodes for $h \in \{0.001446, 0.0018, 0.00195\}$ and is stable for $h \in \{0.00185, 0.0012, 0.0016\}$.

In Figures \ref{pl:explicit_angles} and \ref{pl:explicit_angles_dot} it can be observerd the aproximation using the explicit RK4 for a value of the stes size which makes the approximation stable, in particular we are taking $h=1\mathrm{e}{-4}$.

\begin{figure}[h]
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/RK4_Proj2/angles}
\caption{Approximation of angles using explicit method}
\label{pl:explicit_angles}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/RK4_Proj2/derivatives}
\caption{Approximation of angles derivatives using explicit method}
\label{pl:explicit_angles_dot}
\end{minipage}
\end{figure}

\chapter*{Project 3}

In this project we consider an initial value problem of the form
\begin{align*}
	M \ddot{u}+C\dot{u}+Ku = f(t) \\
	u(0) = u_0 \\
	\dot{u}(0) = v_0
\end{align*}
for this we implemented an Assimulo problem class, the HHT solver and the Newmark implicit and explicit solvers. The explicit solver was tested on the pendulum 

\section*{The pendulum revisited}

\section*{An elastic beam}

In the second part of the project we tested the implicit Newmark and the HHT method on a discretised beam as seen in figure \ref{pl:beam_soln_initial}. The beam is displaced by a force until it is deformed as in figure \ref{pl:beam_soln_position1}. With time it then swings back and forth between the positions in figures \ref{pl:beam_soln_initial}-\ref{pl:beam_soln_position3}. Figure \ref{pl:beam_soln_displacement} shows the displacement of the tip of the beam in dependence of time.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_1.pdf}
\caption{Beam position at $t\approx0$.}
\label{pl:beam_soln_initial}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_8.pdf}
\caption{Beam position at $t\approx1.7$.}
\label{pl:beam_soln_position1}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_10.pdf}
\caption{Beam position at $t\approx2.4$.}
\label{pl:beam_soln_position2}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_12.pdf}
\caption{Beam position at $t\approx2.7$.}
\label{pl:beam_soln_position3}
\end{minipage}

\end{figure} We can calculate the elastic and kinetic energies according to the formulas
\begin{align*}
	E_{\text{kin}} = \frac{1}{2}v^\top Mv
	\qquad\qquad E_{\text{elast}} = \frac{1}{2}u^\top Cu
\end{align*}
which add up to the total energy
\begin{align*}
	E_{\text{tot}}=E_{\text{kin}}+E_{\text{elast}}
\end{align*}
The development of the energy of the system can be seen in figure \ref{pl:beam_soln_energy}. Here we see in particular that after the initial application of an external force to the system the energy stays almost constant. As in project 1 the variance of the total energy serves as a measure of the instability of the solver. Here we calculate this variance only for the latter $4/5$ of the simulation because in the first part the applied force changes the total energy.
With an ideal solver this quantity vanishes.

\begin{figure}[h]
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_905.pdf}
\caption{Displacement of the tip of the beam of the solution.}
\label{pl:beam_soln_displacement}
\end{minipage}
\hfill
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_902.pdf}
\caption{Energy in dependence of time of the solution.}
\label{pl:beam_soln_energy}
\end{minipage}
\end{figure}

\subsubsection*{A brief comparison of solvers}

In a first experiment we compare the performance of our implementation of the HHT solver and the implicit Euler solver from Assimulo. The HHT method was applied with the parameter $\alpha=0$ and the stepsize was the same for all methods. The results are plotted in figure \ref{tb:beam_exp_solverComparison}. Here one sees that for all solvers the variance of the total energy is small. There is however a big difference in the performance of the various methods. On my computer the implicit Euler solver takes roughly two orders of magnitude longer than the HHT method.

\begin{figure}[h]
\centering
\input{../Plots/Tables/Statistics_beam_solvers.tex}
\caption{Performance of various solvers for the beam problem}
\label{tb:beam_exp_solverComparison}
\end{figure}


\subsubsection*{Testing the implicit Newmark solver}

In a second experiment we test the dependence of the implicit Newmark method on the parameters $\beta$ and $\gamma$ whereby we keep the parameter $\alpha=0$ and the stepsize $h=0.05$ constant. The variance of the total energy can be seen in figure \ref{pl:beam_exp_betagamma_alpha0}. It should be noted that we cut off the value of the variance of the total energy at $10^2$ because this value shows that the solution is instable for the specific choice of $\beta$ and $\gamma$. One can observe that the problem becomes more stable with decreasing $\beta$ and and that the region of stability increases with decreasing $\gamma$. Figure \ref{pl:beam_exp_betagamma_instable1} shows what happens if we leave the region of stability.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_910.pdf}
\caption{Dependence of the variance of the total energy on the parameters $\beta$ and $\gamma$ for $\alpha=0$.}
\label{pl:beam_exp_betagamma_alpha0}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_50.pdf}
\caption{The parameter choice $\beta=0.25$ and $\gamma=0.7$ yields rather peculiar beam configurations.}
\label{pl:beam_exp_betagamma_instable1}
\end{minipage}
\end{figure}

\subsubsection*{Testing the HHT solver}

In a final experiment we test the dependence of the HHT solver on the parameter $\alpha$. The variance of the total energy can be seen in figure \ref{pl:beam_exp_HHTalpha} where it is noticable that the value decreases as $\alpha$ increases albeit from a small level. To make it more visible what is happening we set the step size to $h=1$ and plotted the energies of the solutions for the HHT solver as can be seen in figures \ref{pl:beam_exp_HHTalpha_0} and \ref{pl:beam_exp_HHTalpha_minimal}. Here the parameter $\alpha=-1/3$ acts in a dampening manner in comparison to the plot for $\alpha=0$. It is also astonishing that despite the very rough step size the energy plot for $\alpha=0$ shares many features of the solution with a more refined step size, like the almost constant total energy and the eventual periodic behaviour of the kinetic and elastic energies.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_920.pdf}
\caption{Dependence of the variance of the total energy on the parameter $\alpha$.}
\label{pl:beam_exp_HHTalpha}
\end{minipage}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_900.pdf}
\caption{Energy f1or the HHT method with $\alpha=0$ and stepsize $h=1$.}
\label{pl:beam_exp_HHTalpha_0}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Project3_main/Figure_901.pdf}
\caption{Energy for the HHT method with $\alpha=-1/3$ and stepsize $h=1$.}
\label{pl:beam_exp_HHTalpha_minimal}
\end{minipage}
\end{figure}

\section*{The elastic pendulum}

In this section we will use the developed methods to solve the problem of the elastic spring presented in Project 1. In particular we will compare the performance of different explicit methods solving the problem. The method we will be comparing are Newmark Explicit against Explicit Euler and Newmark Explicit againts RK4.
\bigbreak
It is common to both methods that the simulations start at the same point and rapidly drift away. This drif will continuate in different ways depending on the method tested.

\subsubsection*{Newmark - RK4}

This is probably the most interesting of both cases given that RK4 doesn't lose stability as fast as Explici Euler does. As mentioned, the distance between the approximations will drift away in an oscillating fashion with an increasing amplitude. This can be observed in Figure \ref{pl:new_rk4_k=10} for $y$ and in Figure \ref{pl:new_rk4_k=10_dot} for $\dot{y}$.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Diff_Proj3/Real/New-RK4_k=1E1_t=1E5_y}
\caption{Difference of $y$ with $h=0.01$ and $k=10$.}
\label{pl:new_rk4_k=10}
\end{minipage}
\hfill	
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Diff_Proj3/Real/New-RK4_k=1E1_t=1E5_ydot}
\caption{Difference of $\dot{y}$ with $h=0.01$ and $k=10$.}\label{pl:new_rk4_k=10_dot}
\end{minipage}
\end{figure}

The amplitude of the oscillations will eventualy converge to a value which appears to depend on the value of $k$. For bigger values of $k$ the amplitude appeats to converge faster. Convergence of the amplitudes can be observed in Figure \ref{pl:new_rk4_k=1000} for $y$ and in Figure \ref{pl:new_rk4_k=1000_dot} for $\dot{y}$.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Diff_Proj3/Real/New-RK4_k=1E3_t=1E5_y}
\caption{Difference of $y$ with $h=0.01$ and $k=1\mathrm{e}{3}$.}
\label{pl:new_rk4_k=1000}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Diff_Proj3/Real/New-RK4_k=1E3_t=1E5_ydot}
\caption{Difference of $\dot{y}$ with $h=0.01$ and $k=1\mathrm{e}{3}$.}
\label{pl:new_rk4_k=1000_dot}
\end{minipage}
\end{figure}

With regards to performance, Newmark's method is faster than RK4 being for different values of $h$ and $t_f$ approximately $c$ times faster having $c$ order of magnitude 0.

\subsubsection*{Newmark - Euler}
Not much can be said about the relation between Newmark's and Euler's methods given the unstable nature of Explicit Euler. It is common to see how Euler's method explodes to infinity while Newmark's remains stable. This can be observed in Figure \ref{pl:new_eul}.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../Plots/Diff_Proj3/Real/New-Eul_k=1E1_t=1E3_y}
\caption{Difference of $y$ with $h=0.01$ and $k=10$.}
\label{pl:new_eul}
\end{minipage}
\end{figure}

With regard to performance, Euler's method is faster than Newmark's for different values of $h$ and $t_f$ approximately $c$ times faster having $c$ order of magnitude 0.



\chapter*{Appendix}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\hspace*{-1cm}
\scalebox{0.32}{
\input{../Drawings/Stability_region_for_BDF1.pdf_tex}
}
\caption{Stability region for BDF1, taken from \cite{Stab_BDF}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\hspace*{-1cm}
\scalebox{0.32}{
\input{../Drawings/Stability_region_for_BDF2.pdf_tex}
}
\caption{Stability region for BDF2, taken from \cite{Stab_BDF}}
\end{minipage}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\hspace*{-1cm}
\scalebox{0.32}{
\input{../Drawings/Stability_region_for_BDF3.pdf_tex}
}
\caption{Stability region for BDF3, taken from \cite{Stab_BDF}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\hspace*{-1cm}
\scalebox{0.32}{
\input{../Drawings/Stability_region_for_BDF4.pdf_tex}
}
\caption{Stability region for BDF4, taken from \cite{Stab_BDF}}
\end{minipage}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\hspace*{-1cm}
\scalebox{0.32}{
\input{../Drawings/Stability_region_for_BDF5.pdf_tex}
}
\caption{Stability region for BDF5, taken from \cite{Stab_BDF}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\hspace*{-1cm}
\scalebox{0.32}{
\input{../Drawings/Stability_region_for_BDF6.pdf_tex}
}
\caption{Stability region for BDF6, taken from \cite{Stab_BDF}}
\end{minipage}
\end{figure}

% here come some stability regions


%\nocite{*}
\printbibliography

\end{document}
